{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ee77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, roc_curve, mean_squared_error\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# neural network model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# hyperparameter tuning\n",
    "from bayes_opt import BayesianOptimization # can try Optuna also if it is better\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b8a01",
   "metadata": {},
   "source": [
    "# Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e19317",
   "metadata": {},
   "source": [
    "### Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bf5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain time series for full 270 days\n",
    "l90d = pd.read_csv(\"train_data_l90d_daily_balance.csv\")\n",
    "n180d = pd.read_csv(\"train_data_n180d_daily_balance.csv\")\n",
    "\n",
    "df_270 = pd.concat([l90d, n180d], ignore_index=True).drop_duplicates().sort_values(by=['user_id', 'pt_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fbff6",
   "metadata": {},
   "source": [
    "### User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2610ef46-8fd8-4ad5-8ea5-e2854ecc61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f07a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_data = pd.read_csv(\"user_features_l90_stable_20231111.csv\")\n",
    "growth_data = pd.read_csv(\"user_features_l90_growth_20231111.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83932b15",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ece2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a train-test split with all available features\n",
    "# The split is stratified on the label, meaning the proportion of classes will be similar across train % validation sets\n",
    "# But this doesn't ensure that the WEIGHTS of classes will be similar across train & validation sets\n",
    "# SMOTE is done to deal with the minority positive class\n",
    "\n",
    "def get_X_train_y_train(data):\n",
    "    X = data.drop(columns=['label'])\n",
    "    y = data['label']\n",
    "\n",
    "    # Train-test split on user level\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        stratify=y,\n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=0,\n",
    "                                                       )\n",
    "    \n",
    "    # Store train & validation set users\n",
    "    train_users = X_train.user_id.tolist()\n",
    "    test_users = X_test.user_id.tolist()\n",
    "\n",
    "    # Drop user_id\n",
    "    X_train = X_train.drop(columns=['user_id'])\n",
    "    X_test = X_test.drop(columns=['user_id'])\n",
    "    \n",
    "    # SMOTE to deal with class imbalance\n",
    "    smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_users, test_users\n",
    "\n",
    "\n",
    "stable_X_train, stable_y_train, stable_X_test, stable_y_test, stable_train_users, stable_test_users = get_X_train_y_train(stable_data)\n",
    "growth_X_train, growth_y_train, growth_X_test, growth_y_test, stable_train_users, growth_test_users = get_X_train_y_train(growth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b48886",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4da44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_X_train_test(features_lst, subset_type):\n",
    "    if subset_type.lower() == 'stable':\n",
    "        return stable_X_train[features_lst], stable_X_test[features_lst]\n",
    "    else:\n",
    "        return growth_X_train[features_lst], growth_X_test[features_lst]\n",
    "\n",
    "# Finalise features to use for each model\n",
    "lgbm_features_lst_growth = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_ema_crosses',\n",
    "    'stationary',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn',\n",
    "]\n",
    "\n",
    "\n",
    "lgbm_features_lst_stable = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn'\n",
    "]\n",
    "\n",
    "\n",
    "xgb_features_lst_growth = ['abs_bal_change_std', \n",
    "                        'beta_normalized', \n",
    "                        'deposits',\n",
    "                        'withdrawals', \n",
    "                        'label_by_avg_bal', \n",
    "                        'trend',\n",
    "                        'income',\n",
    "                        'subscription',\n",
    "                        'stat_sig_positive_kendall']\n",
    "\n",
    "xgb_features_lst_stable = ['abs_bal_change_std', \n",
    "                            'beta_normalized',\n",
    "                            'label_by_avg_bal', \n",
    "                            'trend',\n",
    "                           'income']\n",
    "    \n",
    "\n",
    "logreg_features_lst_growth = ['volatility_stdev', 'volatility_cv',\n",
    "                               'abs_bal_change_std', 'trend',\n",
    "                               'deposits', 'num_ema_crosses', \n",
    "                               'num_distinct_recurring_tx', \n",
    "                               'withdrawal_propn']\n",
    "\n",
    "logreg_features_lst_stable = ['growth_coeff', 'abs_bal_change_std',\n",
    "                              'deposits', 'withdrawals', 'ema_7day']\n",
    "\n",
    "mlp_features_lst_growth = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_ema_crosses',\n",
    "    'stationary',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn',\n",
    "]\n",
    "\n",
    "mlp_features_lst_stable = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn'\n",
    "]\n",
    "\n",
    "features_lst_growth = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_ema_crosses',\n",
    "    'stationary',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn'       \n",
    "]\n",
    "\n",
    "features_lst_stable = [\n",
    "    'abs_bal_change_std', \n",
    "    'beta_normalized', \n",
    "    'label_by_avg_bal', \n",
    "    'trend',\n",
    "    'num_distinct_recurring_tx',\n",
    "    'recurring_withdrawals',\n",
    "    'withdrawal_propn'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50a0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_X_train_lgbm, stable_X_test_lgbm = get_final_X_train_test(lgbm_features_lst_stable, 'stable')\n",
    "growth_X_train_lgbm, growth_X_test_lgbm = get_final_X_train_test(lgbm_features_lst_growth, 'growth')\n",
    "\n",
    "\n",
    "stable_X_train_xgb, stable_X_test_xgb = get_final_X_train_test(xgb_features_lst_stable, 'stable')\n",
    "growth_X_train_xgb, growth_X_test_xgb = get_final_X_train_test(xgb_features_lst_growth, 'growth')\n",
    "\n",
    "\n",
    "stable_X_train_logreg, stable_X_test_logreg = get_final_X_train_test(logreg_features_lst_stable, 'stable')\n",
    "growth_X_train_logreg, growth_X_test_logreg = get_final_X_train_test(logreg_features_lst_growth, 'growth')\n",
    "\n",
    "stable_X_train_logreg = scale_features(stable_X_train_logreg)\n",
    "stable_X_test_logreg = scale_features(stable_X_test_logreg)\n",
    "growth_X_train_logreg = scale_features(growth_X_train_logreg)\n",
    "growth_X_test_logreg = scale_features(growth_X_test_logreg)\n",
    "\n",
    "stable_X_train_mlp, stable_X_test_mlp = get_final_X_train_test(mlp_features_lst_stable, 'stable')\n",
    "growth_X_train_mlp, growth_X_test_mlp = get_final_X_train_test(mlp_features_lst_growth, 'growth')\n",
    "\n",
    "stable_X_train_mlp = scale_features(stable_X_train_mlp)\n",
    "stable_X_test_mlp = scale_features(stable_X_test_mlp)\n",
    "growth_X_train_mlp = scale_features(growth_X_train_mlp)\n",
    "growth_X_test_mlp = scale_features(growth_X_test_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8ac22",
   "metadata": {},
   "source": [
    "#### **VIF check for autocorrelation between cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e6fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Features       VIF\n",
      "2            deposits  1.797526\n",
      "3         withdrawals  1.758894\n",
      "1  abs_bal_change_std  1.369367\n",
      "0        growth_coeff  1.306846\n",
      "4            ema_7day  1.072925\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_vif(X): \n",
    "\n",
    "    # Create a DataFrame to store the VIF values\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Features\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    # Display the VIF values\n",
    "    print(vif_data.sort_values(by='VIF', ascending=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def show_vif(model_name, subset_type):\n",
    "    model_map = {\n",
    "        'stable': {\n",
    "            'lgbm': stable_X_train_lgbm,\n",
    "            'xgb': stable_X_train_xgb,\n",
    "            'logreg': stable_X_train_logreg,\n",
    "            'mlp': stable_X_train_mlp,\n",
    "        },\n",
    "        'growth': {\n",
    "            'lgbm': growth_X_train_lgbm,\n",
    "            'xgb': growth_X_train_xgb,\n",
    "            'logreg': growth_X_train_logreg,\n",
    "            'mlp': growth_X_train_mlp,\n",
    "        }\n",
    "    }\n",
    "    return get_vif(model_map[subset_type][model_name])\n",
    "\n",
    "# measure multi-collinearity between the features\n",
    "show_vif('logreg', 'stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30729f19",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get metric for a single model\n",
    "def get_metrics_df(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    # Create a dictionary to hold the metrics\n",
    "    metrics_dict = {\n",
    "        'Class': [0, 1],\n",
    "        'Accuracy': accuracy.tolist(),\n",
    "        'Precision': precision.tolist(),\n",
    "        'Recall': recall.tolist(),\n",
    "        'ROC-AUC': roc_auc.tolist(),\n",
    "        'F1-Score': f1.tolist(),\n",
    " \n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    metrics_df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "    # Set the 'Class' column as the index\n",
    "    metrics_df.set_index('Class', inplace=True)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad0b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get metric in a df for all models\n",
    "def get_all_metrics_df(y_pred1, y_pred2, y_pred3, y_pred4, y_test):\n",
    "    model_labels = [\"LGBM\", \"XGB\", \"LogReg\", \"MLP\"]\n",
    "\n",
    "    # Create an empty list to store the metrics DataFrames\n",
    "    all_metrics_dfs = []\n",
    "\n",
    "    # Calculate and store metrics for each model (replace y_pred_modelX and y_test_modelX with your actual data)\n",
    "    for label, (y_pred, y_test) in zip(model_labels, [(y_pred1, y_test), (y_pred2, y_test), (y_pred3, y_test), (y_pred4, y_test)]):\n",
    "        metrics_df = get_metrics_df(y_test, y_pred)\n",
    "        metrics_df[\"Model\"] = label  # Add a 'Model' column to label the metrics\n",
    "        all_metrics_dfs.append(metrics_df)\n",
    "\n",
    "    # Concatenate the metrics DataFrames vertically\n",
    "    all_metrics_df = pd.concat(all_metrics_dfs, axis=0)\n",
    "\n",
    "    # Reset the index to have a continuous index\n",
    "    all_metrics_df.reset_index(inplace=True)\n",
    "\n",
    "    return all_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847bc28",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96fedc-db8d-4505-83d1-23c626e31014",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cee5ae9-4f92-44c1-982d-406bbf28bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LightGBM and XGBoost\n",
    "\n",
    "def get_feature_importance(X_train, best_model):\n",
    "    # Get feature importances\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X_train.columns \n",
    "\n",
    "    # Pair feature names with their importance scores\n",
    "    feature_importance_dict = dict(zip(feature_names, feature_importance))\n",
    "\n",
    "    # Sort feature importance dictionary by values (importance scores)\n",
    "    sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    top_features = []\n",
    "    # Print or use the sorted feature importance\n",
    "    for feature, importance in sorted_feature_importance.items():\n",
    "        print(f\"{feature}: {importance}\")\n",
    "        top_features.append(feature)\n",
    "    \n",
    "\n",
    "def grid_search(parameters, model, X_train, y_train, X_test):\n",
    "    cv_split = StratifiedKFold()\n",
    "\n",
    "    optimised_model = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters, scoring='recall')\n",
    "    optimised_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = optimised_model.predict(X_test)\n",
    "    \n",
    "    best_model = optimised_model.best_estimator_\n",
    "    best_param = optimised_model.best_params_\n",
    "    best_score = optimised_model.best_score_\n",
    "    \n",
    "    print(best_model)\n",
    "    print(best_score)\n",
    "    \n",
    "    get_feature_importance(X_train, best_model)\n",
    "\n",
    "    return optimised_model, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356b7e17-c37c-4c92-baaa-e8ccbf1cfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Log Reg\n",
    "def get_feature_importance1(X_train, best_model):\n",
    "    # Get feature importances\n",
    "    coefficients = best_model.coef_[0]\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "    print(feature_importance)\n",
    "\n",
    "def grid_search1(parameters, model, X_train, y_train, X_test):\n",
    "    cv_split = StratifiedKFold()\n",
    "\n",
    "    optimised_model = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters, scoring='recall')\n",
    "    optimised_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = optimised_model.predict(X_test)\n",
    "\n",
    "    best_model = optimised_model.best_estimator_\n",
    "    best_param = optimised_model.best_params_\n",
    "    best_score = optimised_model.best_score_\n",
    "\n",
    "    print(best_model)\n",
    "    print(best_score)\n",
    "\n",
    "    get_feature_importance1(X_train, best_model)\n",
    "\n",
    "    return optimised_model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d2b35a-e769-481f-bbe7-6b1cdff59ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MLP \n",
    "def bo_simple_nn(X_train: pd.DataFrame, y_train: pd.DataFrame, \n",
    "                 scorer: sklearn.metrics._scorer._PredictScorer, params: dict) -> dict:\n",
    "    \n",
    "    # model training\n",
    "    def nn_cl_bo(neurons, activation, learning_rate,  batch_size, epochs, layers1):\n",
    "        activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                    'elu', 'exponential', 'relu']\n",
    "        neurons = round(neurons)\n",
    "        activation = activationL[round(activation)]\n",
    "        batch_size = round(batch_size)\n",
    "        epochs = round(epochs)\n",
    "        layers1 = round(layers1)\n",
    "\n",
    "        # MLP architecture\n",
    "        def nn_cl_fun():\n",
    "            opt = Adam(learning_rate = learning_rate) # should i optimise for this as well\n",
    "            nn = Sequential()\n",
    "            nn.add(Dense(neurons, input_dim=len(X_train.columns), activation=activation))\n",
    "            for i in range(layers1):\n",
    "                nn.add(Dense(neurons, activation=activation))\n",
    "            nn.add(Dense(1, activation='sigmoid'))\n",
    "            nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall]) #change to recall?\n",
    "            return nn\n",
    "        es = EarlyStopping(monitor='val_recall', mode='max', verbose=0, patience=20) #change to recall?\n",
    "        nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0)\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "        score = cross_val_score(nn, X_train, y_train, scoring=scorer, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "        return score\n",
    "    \n",
    "    # Bayesian Optimisation\n",
    "    nn_bo = BayesianOptimization(nn_cl_bo, params, random_state=111)\n",
    "    nn_bo.maximize(init_points=10, n_iter=10)\n",
    "\n",
    "    return nn_bo.max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cf7b5",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b737941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "\n",
    "    \n",
    "def run_gbm(subset_type):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    model.set_params(verbose=-1) \n",
    "    \n",
    "    parameters = {\n",
    "        \"max_depth\": [10],\n",
    "        \"num_leaves\": [20],\n",
    "        \"learning_rate\": [0.05],\n",
    "        \"n_estimators\": [1000],\n",
    "    }\n",
    "\n",
    "# set of parameters for grid search optimisation\n",
    "#     parameters = {\n",
    "#         \"max_depth\": [10, 20, 30],\n",
    "#         \"num_leaves\": [10, 20, 30],\n",
    "#         \"learning_rate\": [0.02, 0.05],\n",
    "#         \"n_estimators\": [1000, 1200],\n",
    "#     }\n",
    "    \n",
    "    if subset_type == 'stable':\n",
    "        model, y_pred = grid_search(parameters, model, stable_X_train_lgbm, stable_y_train, stable_X_test_lgbm)\n",
    "    else:\n",
    "        model, y_pred = grid_search(parameters, model, growth_X_train_lgbm, growth_y_train, growth_X_test_lgbm)\n",
    "        \n",
    "    return model, y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64dd1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(learning_rate=0.05, max_depth=10, n_estimators=1000,\n",
      "               num_leaves=20, random_state=0, verbose=-1)\n",
      "0.9854207142662613\n",
      "abs_bal_change_std: 5249\n",
      "beta_normalized: 3925\n",
      "withdrawal_propn: 3780\n",
      "num_distinct_recurring_tx: 3762\n",
      "label_by_avg_bal: 1595\n",
      "trend: 600\n",
      "recurring_withdrawals: 89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935388</td>\n",
       "      <td>0.995030</td>\n",
       "      <td>0.939591</td>\n",
       "      <td>0.660585</td>\n",
       "      <td>0.966516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.935388</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.660585</td>\n",
       "      <td>0.081690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.935388   0.995030  0.939591  0.660585  0.966516\n",
       "1      0.935388   0.045741  0.381579  0.660585  0.081690"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_stable, stable_y_pred_gbm = run_gbm('stable')\n",
    "lgbm_metrics_stable = get_metrics_df(stable_y_test, stable_y_pred_gbm)\n",
    "lgbm_metrics_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250648a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(learning_rate=0.05, max_depth=10, n_estimators=1000,\n",
      "               num_leaves=20, random_state=0, verbose=-1)\n",
      "0.9618241903502975\n",
      "abs_bal_change_std: 5524\n",
      "beta_normalized: 4326\n",
      "num_distinct_recurring_tx: 3045\n",
      "withdrawal_propn: 2436\n",
      "num_ema_crosses: 1491\n",
      "label_by_avg_bal: 1254\n",
      "stationary: 424\n",
      "trend: 412\n",
      "recurring_withdrawals: 88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849073</td>\n",
       "      <td>0.982721</td>\n",
       "      <td>0.853971</td>\n",
       "      <td>0.814998</td>\n",
       "      <td>0.913833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.849073</td>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>0.814998</td>\n",
       "      <td>0.392501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.849073   0.982721  0.853971  0.814998  0.913833\n",
       "1      0.849073   0.262680  0.776025  0.814998  0.392501"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_growth, growth_y_pred_gbm = run_gbm('growth')\n",
    "lgbm_metrics_growth = get_metrics_df(growth_y_test, growth_y_pred_gbm)\n",
    "lgbm_metrics_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffd5c3",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2454dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XGB\n",
    "def run_xgb(subset_type):\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",  \n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "# set of parameters for grid search optimisation   \n",
    "#     parameters = {\n",
    "#         \"max_depth\": [10, 20, 30],\n",
    "#         \"learning_rate\": [0.05, 0.06, 0.07],\n",
    "#         \"n_estimators\": [500, 1000, 1500],\n",
    "#     }\n",
    "\n",
    "    #optimised params\n",
    "    parameters = {\n",
    "        \"max_depth\": [10],\n",
    "        \"learning_rate\": [0.05],\n",
    "        \"n_estimators\": [500],\n",
    "        }\n",
    "    \n",
    "    if subset_type == 'stable':\n",
    "        model, y_pred = grid_search(parameters, model, stable_X_train_xgb, stable_y_train, stable_X_test_xgb)\n",
    "    else:\n",
    "        model, y_pred = grid_search(parameters, model, growth_X_train_xgb, growth_y_train, growth_X_test_xgb)\n",
    "        \n",
    "    return model, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e35df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=0, ...)\n",
      "0.94003539379484\n",
      "label_by_avg_bal: 0.8231279850006104\n",
      "abs_bal_change_std: 0.06988933682441711\n",
      "trend: 0.057336147874593735\n",
      "beta_normalized: 0.026339510455727577\n",
      "income: 0.023306960240006447\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875434</td>\n",
       "      <td>0.996260</td>\n",
       "      <td>0.877783</td>\n",
       "      <td>0.721786</td>\n",
       "      <td>0.933277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875434</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.721786</td>\n",
       "      <td>0.064036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.875434   0.996260  0.877783  0.721786  0.933277\n",
       "1      0.875434   0.033938  0.565789  0.721786  0.064036"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_stable, stable_y_pred_xgb = run_xgb('stable')\n",
    "xgb_metrics_stable = get_metrics_df(stable_y_test, stable_y_pred_xgb)\n",
    "xgb_metrics_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0458beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=0, ...)\n",
      "0.9531262392597488\n",
      "withdrawals: 0.7746371626853943\n",
      "trend: 0.0689874067902565\n",
      "stat_sig_positive_kendall: 0.05718370899558067\n",
      "deposits: 0.029127534478902817\n",
      "label_by_avg_bal: 0.024625379592180252\n",
      "abs_bal_change_std: 0.021099120378494263\n",
      "beta_normalized: 0.014100583270192146\n",
      "subscription: 0.005332460161298513\n",
      "income: 0.004906768444925547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.846992</td>\n",
       "      <td>0.982206</td>\n",
       "      <td>0.852173</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.912581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846992</td>\n",
       "      <td>0.258749</td>\n",
       "      <td>0.769716</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.387302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.846992   0.982206  0.852173  0.810945  0.912581\n",
       "1      0.846992   0.258749  0.769716  0.810945  0.387302"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_growth, growth_y_pred_xgb = run_xgb('growth')\n",
    "xgb_metrics_growth = get_metrics_df(growth_y_test, growth_y_pred_xgb)\n",
    "xgb_metrics_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd6a75",
   "metadata": {},
   "source": [
    "### Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e244ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_log_reg(subset_type):\n",
    "    model = LogisticRegression(\n",
    "        max_iter=10000,  # Maximum number of iterations\n",
    "        random_state=0,  # Random seed for reproducibility\n",
    "        solver = 'saga'\n",
    "    )\n",
    "    \n",
    "# set of parameters for grid search optimisation\n",
    "#     parameters = {\n",
    "#         \"C\": [0.0001, 0.001, 0.01],  # Try different values for C\n",
    "#         \"penalty\": ['l1', 'l2']  # Try both L1 and L2 regularization\n",
    "#     }\n",
    "\n",
    "    #optimised value\n",
    "    parameters = {\n",
    "        \"C\": [0.01],\n",
    "        \"penalty\": ['l1']\n",
    "    }\n",
    "    \n",
    "    if subset_type == 'stable':\n",
    "        model, y_pred = grid_search1(parameters, model, stable_X_train_logreg, stable_y_train, stable_X_test_logreg)\n",
    "    else:\n",
    "        model, y_pred = grid_search1(parameters, model, growth_X_train_logreg, growth_y_train, growth_X_test_logreg)\n",
    "        \n",
    "    return model, y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a86a4d1-530c-4a7e-bf57-a556a9b316d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, max_iter=10000, penalty='l1', random_state=0,\n",
      "                   solver='saga')\n",
      "0.9149711956843165\n",
      "              Feature  Importance\n",
      "0        growth_coeff    0.040391\n",
      "1  abs_bal_change_std    0.122138\n",
      "3         withdrawals    1.135660\n",
      "4            ema_7day    3.493188\n",
      "2            deposits    3.842446\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.512734</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.509136</td>\n",
       "      <td>0.747989</td>\n",
       "      <td>0.674694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512734</td>\n",
       "      <td>0.015027</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.747989</td>\n",
       "      <td>0.029603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.512734   0.999804  0.509136  0.747989  0.674694\n",
       "1      0.512734   0.015027  0.986842  0.747989  0.029603"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_stable, stable_y_pred_logreg = run_log_reg('stable')\n",
    "logreg_metrics_stable = get_metrics_df(stable_y_test, stable_y_pred_logreg)\n",
    "logreg_metrics_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad869fa-76f5-4168-82da-b5b3b4cb020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, max_iter=10000, penalty='l1', random_state=0,\n",
      "                   solver='saga')\n",
      "0.8833840052875083\n",
      "                     Feature  Importance\n",
      "2         abs_bal_change_std    0.006535\n",
      "0           volatility_stdev    0.021067\n",
      "1              volatility_cv    0.284424\n",
      "3                      trend    0.371020\n",
      "6  num_distinct_recurring_tx    0.596324\n",
      "7           withdrawal_propn    0.666639\n",
      "4                   deposits    1.040451\n",
      "5            num_ema_crosses    1.854386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.62858</td>\n",
       "      <td>0.996521</td>\n",
       "      <td>0.605795</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>0.753518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.62858</td>\n",
       "      <td>0.141409</td>\n",
       "      <td>0.968454</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>0.246785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0       0.62858   0.996521  0.605795  0.787124  0.753518\n",
       "1       0.62858   0.141409  0.968454  0.787124  0.246785"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_growth, growth_y_pred_logreg = run_log_reg('growth')\n",
    "logreg_metrics_growth = get_metrics_df(growth_y_test, growth_y_pred_logreg)\n",
    "logreg_metrics_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75668cb",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dfa1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71086c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the bayesian optimisation function for MLP if you want to find the best parameters\n",
    "\n",
    "# params_simple_nn = {\n",
    "#     'neurons': (10, 100),\n",
    "#     'activation':(0, 8),\n",
    "#     'learning_rate':(0.01, 1),\n",
    "#     'batch_size':(200, 1000),\n",
    "#     'epochs':(20, 100),\n",
    "#     'layers1':(1,5)\n",
    "# }\n",
    "\n",
    "# scorer_rec = make_scorer(recall_score)\n",
    "\n",
    "# # this is to run the function\n",
    "# bo_simple_nn(X_train, y_train, scorer_rec, params_simple_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7472f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp function with the best parameters after doing bayesian optimisation \n",
    "def run_mlp(subset_type):\n",
    "    \n",
    "    if subset_type == 'stable':\n",
    "        learning_rate = 0.04374\n",
    "        neurons = 95\n",
    "        layers1 = 4\n",
    "        epochs = 94\n",
    "        batch_size = 958\n",
    "        activation = 'softsign'\n",
    "\n",
    "        def nn_cl_fun():\n",
    "            opt = Adam(learning_rate = learning_rate)\n",
    "            nn = Sequential()\n",
    "            nn.add(Dense(neurons, input_dim=len(stable_X_train_mlp.columns), activation=activation))\n",
    "            for i in range(layers1):\n",
    "                nn.add(Dense(neurons, activation=activation))\n",
    "            nn.add(Dense(1, activation='sigmoid'))\n",
    "            nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall]) \n",
    "            return nn\n",
    "        es = EarlyStopping(monitor='val_recall', mode='max', verbose=0, patience=20)\n",
    "        nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0)\n",
    "        nn.fit(stable_X_train_mlp, stable_y_train, validation_data=(stable_X_test_mlp, stable_y_test), verbose=0)\n",
    "        y_pred = nn.predict(stable_X_test_mlp)\n",
    "        return nn, y_pred\n",
    "    \n",
    "    else:\n",
    "        learning_rate = 0.01\n",
    "        neurons = 68\n",
    "        layers1 = 5\n",
    "        epochs = 100\n",
    "        batch_size = 973\n",
    "        activation = 'softsign'\n",
    "\n",
    "        def nn_cl_fun():\n",
    "            opt = Adam(learning_rate = learning_rate)\n",
    "            nn = Sequential()\n",
    "            nn.add(Dense(neurons, input_dim=len(growth_X_train_mlp.columns), activation=activation))\n",
    "            for i in range(layers1):\n",
    "                nn.add(Dense(neurons, activation=activation))\n",
    "            nn.add(Dense(1, activation='sigmoid'))\n",
    "            nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', recall])\n",
    "            return nn\n",
    "        es = EarlyStopping(monitor='val_recall', mode='max', verbose=0, patience=20)\n",
    "        nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0)\n",
    "        nn.fit(growth_X_train_mlp, growth_y_train, validation_data=(growth_X_test_mlp, growth_y_test), verbose=0)\n",
    "        y_pred = nn.predict(growth_X_test_mlp)\n",
    "        return nn, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "601e85dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.804182</td>\n",
       "      <td>0.997401</td>\n",
       "      <td>0.804793</td>\n",
       "      <td>0.764239</td>\n",
       "      <td>0.890805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.804182</td>\n",
       "      <td>0.027363</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.764239</td>\n",
       "      <td>0.052733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.804182   0.997401  0.804793  0.764239  0.890805\n",
       "1      0.804182   0.027363  0.723684  0.764239  0.052733"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_stable, stable_y_pred_mlp = run_mlp('stable')\n",
    "mlp_metrics_stable = get_metrics_df(stable_y_test, stable_y_pred_mlp)\n",
    "mlp_metrics_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bfc0c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752651</td>\n",
       "      <td>0.985764</td>\n",
       "      <td>0.746854</td>\n",
       "      <td>0.792985</td>\n",
       "      <td>0.849838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.752651</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.839117</td>\n",
       "      <td>0.792985</td>\n",
       "      <td>0.298876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall   ROC-AUC  F1-Score\n",
       "Class                                                   \n",
       "0      0.752651   0.985764  0.746854  0.792985  0.849838\n",
       "1      0.752651   0.181818  0.839117  0.792985  0.298876"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_growth, growth_y_pred_mlp = run_mlp('growth')\n",
    "mlp_metrics_growth = get_metrics_df(growth_y_test, growth_y_pred_mlp)\n",
    "mlp_metrics_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418dd3e",
   "metadata": {},
   "source": [
    "### Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable Subset\n",
      "   Class  Accuracy  Precision    Recall   ROC-AUC  F1-Score   Model\n",
      "0      0  0.935388   0.995030  0.939591  0.660585  0.966516    LGBM\n",
      "1      1  0.935388   0.045741  0.381579  0.660585  0.081690    LGBM\n",
      "2      0  0.875434   0.996260  0.877783  0.721786  0.933277     XGB\n",
      "3      1  0.875434   0.033938  0.565789  0.721786  0.064036     XGB\n",
      "4      0  0.512734   0.999804  0.509136  0.747989  0.674694  LogReg\n",
      "5      1  0.512734   0.015027  0.986842  0.747989  0.029603  LogReg\n",
      "6      0  0.804182   0.997401  0.804793  0.764239  0.890805     MLP\n",
      "7      1  0.804182   0.027363  0.723684  0.764239  0.052733     MLP\n",
      "\n",
      "Growth Subset\n",
      "   Class  Accuracy  Precision    Recall   ROC-AUC  F1-Score   Model\n",
      "0      0  0.849073   0.982721  0.853971  0.814998  0.913833    LGBM\n",
      "1      1  0.849073   0.262680  0.776025  0.814998  0.392501    LGBM\n",
      "2      0  0.846992   0.982206  0.852173  0.810945  0.912581     XGB\n",
      "3      1  0.846992   0.258749  0.769716  0.810945  0.387302     XGB\n",
      "4      0  0.628580   0.996521  0.605795  0.787124  0.753518  LogReg\n",
      "5      1  0.628580   0.141409  0.968454  0.787124  0.246785  LogReg\n",
      "6      0  0.752651   0.985764  0.746854  0.792985  0.849838     MLP\n",
      "7      1  0.752651   0.181818  0.839117  0.792985  0.298876     MLP\n"
     ]
    }
   ],
   "source": [
    "stable_all_metrics_df = get_all_metrics_df(stable_y_pred_gbm, stable_y_pred_xgb, stable_y_pred_logreg, stable_y_pred_mlp, stable_y_test)\n",
    "growth_all_metrics_df = get_all_metrics_df(growth_y_pred_gbm, growth_y_pred_xgb, growth_y_pred_logreg, growth_y_pred_mlp, growth_y_test)\n",
    "\n",
    "print(\"Stable Subset\")\n",
    "print(stable_all_metrics_df)\n",
    "print(\"\")\n",
    "print(\"Growth Subset\")\n",
    "print(growth_all_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82094a98",
   "metadata": {},
   "source": [
    "# Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a2a1e",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b26e6d",
   "metadata": {},
   "source": [
    "### Growth subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "129b4cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class    Recall\n",
      "0  Non-Growth  0.864651\n",
      "1      Growth  0.742902\n"
     ]
    }
   ],
   "source": [
    "#choose soft or hard voting\n",
    "voting_ensemble = VotingClassifier(estimators=[\n",
    "    ('XGBoost', xgb_growth),\n",
    "    ('LightGBM', gbm_growth),\n",
    "    ('Logistic Regression', log_reg_growth),\n",
    "    ('MLP', mlp_growth),\n",
    "], voting='soft') \n",
    "\n",
    "voting_ensemble.fit(growth_X_train[features_lst_growth], growth_y_train)\n",
    "\n",
    "# Make predictions with the VotingClassifier\n",
    "growth_y_pred_voting = voting_ensemble.predict(growth_X_test[features_lst_growth])\n",
    "\n",
    "recall = recall_score(growth_y_test, growth_y_pred_voting, average=None)\n",
    "\n",
    "recall_df = pd.DataFrame({\n",
    "    'Class': ['Non-Growth', 'Growth'],\n",
    "    'Recall': [recall[0], recall[1]]\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(recall_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522eec18",
   "metadata": {},
   "source": [
    "### Stable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "63d87c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class    Recall\n",
      "0  Non-Stable  0.956665\n",
      "1      Stable  0.342105\n"
     ]
    }
   ],
   "source": [
    "#choose soft or hard voting\n",
    "voting_ensemble = VotingClassifier(estimators=[\n",
    "    ('XGBoost', xgb_stable),\n",
    "    ('LightGBM', gbm_stable),\n",
    "    ('Logistic Regression', log_reg_stable),\n",
    "#     ('MLP', mlp_stable),\n",
    "], voting='soft') \n",
    "\n",
    "voting_ensemble.fit(stable_X_train[features_lst_stable], stable_y_train)\n",
    "\n",
    "# Make predictions with the VotingClassifier\n",
    "stable_y_pred_voting = voting_ensemble.predict(stable_X_test[features_lst_stable])\n",
    "recall = recall_score(stable_y_test, stable_y_pred_voting, average=None)\n",
    "\n",
    "recall_df = pd.DataFrame({\n",
    "    'Class': ['Non-Stable', 'Stable'],\n",
    "    'Recall': [recall[0], recall[1]]\n",
    "})\n",
    "\n",
    "print(recall_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae1c92",
   "metadata": {},
   "source": [
    "## Stacking (for heterogenous models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e3455",
   "metadata": {},
   "source": [
    "### Growth subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1da12cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class    Recall\n",
      "0  Non-Growth  0.880723\n",
      "1      Growth  0.708202\n"
     ]
    }
   ],
   "source": [
    "# Create a stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_growth),\n",
    "        ('gbm', gbm_growth),\n",
    "        ('log_reg', log_reg_growth),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),  # You can use a different final estimator if desired\n",
    "    stack_method='auto'  # Automatically select the best method (can be 'auto', 'predict_proba', or 'decision_function')\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "# Define a new set of features for the stacking clf\n",
    "stacking_clf.fit(growth_X_train[features_lst_growth], growth_y_train)\n",
    "\n",
    "# Make predictions\n",
    "growth_y_pred_stacking = stacking_clf.predict(growth_X_test[features_lst_growth])\n",
    "\n",
    "recall = recall_score(growth_y_test, growth_y_pred_stacking, average=None)\n",
    "recall_df = pd.DataFrame({\n",
    "    'Class': ['Non-Growth', 'Growth'],\n",
    "    'Recall': [recall[0], recall[1]]\n",
    "})\n",
    "\n",
    "print(recall_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904cee2",
   "metadata": {},
   "source": [
    "### Stable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b3874f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class    Recall\n",
      "0  Non-Stable  0.962956\n",
      "1      Stable  0.328947\n"
     ]
    }
   ],
   "source": [
    "# Create a stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_stable),\n",
    "        ('gbm', gbm_stable),\n",
    "        ('log_reg', log_reg_stable),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),  # You can use a different final estimator if desired\n",
    "    stack_method='auto'  # Automatically select the best method (can be 'auto', 'predict_proba', or 'decision_function')\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "# Define a new set of features for the stacking clf\n",
    "stacking_clf.fit(stable_X_train[features_lst_stable], stable_y_train)\n",
    "\n",
    "# Make predictions\n",
    "stable_y_pred_stacking = stacking_clf.predict(stable_X_test[features_lst_stable])\n",
    "\n",
    "recall = recall_score(stable_y_test, stable_y_pred_stacking, average=None)\n",
    "recall_df = pd.DataFrame({\n",
    "    'Class': ['Non-Stable', 'Stable'],\n",
    "    'Recall': [recall[0], recall[1]]\n",
    "})\n",
    "\n",
    "print(recall_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6323c68",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0e4bb",
   "metadata": {},
   "source": [
    "## Get validation set users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775a4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns 1 user-level dataframe of validation set (10k users)\n",
    "def get_results_df(test_users, y_test, y_pred, user_features):\n",
    "    \n",
    "    # Create a DataFrame with validation set users, their true labels, and predicted labels\n",
    "    results_df = pd.DataFrame({\n",
    "        'user_id': test_users, \n",
    "        'true_label': y_test,\n",
    "        'predicted_label': y_pred\n",
    "    })\n",
    "\n",
    "    results_df = results_df.merge(user_features, on='user_id', how='left')\n",
    "    \n",
    "    # Weight of each user relative to the 10k validation set users (not entire portfolio of 50k users)\n",
    "    results_df['weight'] = results_df['avg_balance'] / np.sum(results_df['avg_balance'])\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "# This function returns 2 user-level dataframes\n",
    "# It splits the df generated from the above function into\n",
    "# 1) user-level dataframe of actual subset\n",
    "# 2) user-level dataframe of predicted subset\n",
    "def get_subset_pred_true(results_df):\n",
    "    \n",
    "    ####################\n",
    "    # Predicted subset #\n",
    "    ####################\n",
    "    subset_pred = results_df[results_df['predicted_label'] == 1].copy()\n",
    "    \n",
    "    # calculate weight relative to subset & obtain weighted stability\n",
    "    subset_pred['subset_weight'] = subset_pred['avg_balance'] / subset_pred['avg_balance'].sum()\n",
    "    subset_pred['weighted_stability'] = subset_pred['subset_weight'] * subset_pred['stability_index']\n",
    "    \n",
    "    #################\n",
    "    # Actual subset #\n",
    "    #################\n",
    "    subset_true = results_df[results_df['true_label'] == 1].copy()\n",
    "    \n",
    "    # calculate weight relative to subset & obtain weighted stability\n",
    "    subset_true['subset_weight'] = subset_true['avg_balance'] / subset_true['avg_balance'].sum()\n",
    "    subset_true['weighted_stability'] = subset_true['subset_weight'] * subset_true['stability_index']\n",
    "    \n",
    "    return subset_pred, subset_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57b41cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "growth_results_df_gbm = get_results_df(growth_test_users, growth_y_test, growth_y_pred_gbm, growth_data)\n",
    "stable_results_df_gbm = get_results_df(stable_test_users, stable_y_test, stable_y_pred_gbm, stable_data)\n",
    "\n",
    "growth_pred_gbm, growth_true_gbm = get_subset_pred_true(growth_results_df_gbm)\n",
    "stable_pred_gbm, stable_true_gbm = get_subset_pred_true(stable_results_df_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c4182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "growth_results_df_xgb = get_results_df(growth_test_users, growth_y_test, growth_y_pred_xgb, growth_data)\n",
    "stable_results_df_xgb = get_results_df(stable_test_users, stable_y_test, stable_y_pred_xgb, stable_data)\n",
    "\n",
    "growth_pred_xgb, growth_true_xgb = get_subset_pred_true(growth_results_df_xgb)\n",
    "stable_pred_xgb, stable_true_xgb = get_subset_pred_true(stable_results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1a6487c-bcf1-40bf-9fef-eb9adae52655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg\n",
    "growth_results_df_logreg = get_results_df(growth_test_users, growth_y_test, growth_y_pred_logreg, growth_data)\n",
    "stable_results_df_logreg = get_results_df(stable_test_users, stable_y_test, stable_y_pred_logreg, stable_data)\n",
    "\n",
    "growth_pred_logreg, growth_true_logreg = get_subset_pred_true(growth_results_df_logreg)\n",
    "stable_pred_logreg, stable_true_logreg = get_subset_pred_true(stable_results_df_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4e2ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "growth_results_df_mlp = get_results_df(growth_test_users, growth_y_test, growth_y_pred_mlp, growth_data)\n",
    "stable_results_df_mlp = get_results_df(stable_test_users, stable_y_test, stable_y_pred_mlp, stable_data)\n",
    "\n",
    "growth_pred_mlp, growth_true_mlp = get_subset_pred_true(growth_results_df_mlp)\n",
    "stable_pred_mlp, stable_true_mlp = get_subset_pred_true(stable_results_df_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a08eb064-c061-4aa0-badf-5503009de1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "growth_results_df_voting = get_results_df(growth_test_users, growth_y_test, growth_y_pred_voting, growth_data)\n",
    "stable_results_df_voting = get_results_df(stable_test_users, stable_y_test, stable_y_pred_voting, stable_data)\n",
    "\n",
    "growth_pred_voting, growth_true_voting = get_subset_pred_true(growth_results_df_voting)\n",
    "stable_pred_voting, stable_true_voting = get_subset_pred_true(stable_results_df_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "be29069c-f04c-4f3e-844c-26f1fe139c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "growth_results_df_stacking = get_results_df(growth_test_users, growth_y_test, growth_y_pred_stacking, growth_data)\n",
    "stable_results_df_stacking = get_results_df(stable_test_users, stable_y_test, stable_y_pred_stacking, stable_data)\n",
    "\n",
    "growth_pred_stacking, growth_true_stacking = get_subset_pred_true(growth_results_df_stacking)\n",
    "stable_pred_stacking, stable_true_stacking = get_subset_pred_true(stable_results_df_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad99a0e",
   "metadata": {},
   "source": [
    "## Filtering after prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb258e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_predicted_growth_subset(growth_pred1):\n",
    "    \n",
    "    # Filter for users who exhibit positive trend\n",
    "    growth_pred1 = growth_pred1[(growth_pred1['stat_sig_positive_kendall'] == 1) | (growth_pred1['trend'] == 2)].copy()\n",
    "    \n",
    "    # Calculate growth rate\n",
    "    balance_growth_rate = (growth_pred1['last_day_balance'] - growth_pred1['avg_balance'])/growth_pred1['avg_balance']\n",
    "    \n",
    "    # growth rate will be NaN if avg balance = 0\n",
    "    # For such cases we use growth coefficient as proxy for growth rate\n",
    "    growth_pred1['balance_growth_rate'] = np.where(growth_pred1['avg_balance'] != 0,\n",
    "                                                   balance_growth_rate, \n",
    "                                                   growth_pred1['growth_coeff'])\n",
    "    \n",
    "    growth_pred1_sorted = growth_pred1.sort_values(by=['balance_growth_rate'], ascending=False)\n",
    "\n",
    "    # Filter for 5% of subset weight\n",
    "    growth_pred1_sorted['cumulative_weight'] = growth_pred1_sorted['weight'].cumsum()\n",
    "    growth_pred1_final = growth_pred1_sorted[growth_pred1_sorted['cumulative_weight'] <= 0.05].copy()\n",
    "    \n",
    "    final_weight = growth_pred1_final['weight'] / growth_pred1_final['weight'].sum()\n",
    "    final_weighted_stability = final_weight * growth_pred1_final['stability_index']\n",
    "    \n",
    "    growth_pred1_final['final_weight'] = final_weight\n",
    "    growth_pred1_final['weighted_stability'].update(final_weighted_stability)\n",
    "    \n",
    "    print(growth_pred1_final.true_label.value_counts())\n",
    "    \n",
    "    return growth_pred1_final\n",
    "\n",
    "\n",
    "def filter_predicted_stable_subset(stable_pred1):\n",
    "\n",
    "    stable_pred1 = stable_pred1[(stable_pred1['stat_sig_positive_kendall'] == 1) | (stable_pred1['trend'] == 2) | ((stable_pred1['stationary'] == 1) & (stable_pred1['trend'] == 1))].copy()\n",
    "    \n",
    "    # Can't sort by stability index because we've already established that \n",
    "    # the most stable users for the next 180 days were not the most stable for their first 90 days\n",
    "    stable_pred1_sorted = stable_pred1.sort_values(by=['trend', 'stat_sig_positive_kendall', 'stationary'], ascending=False)\n",
    "    \n",
    "    # Filter for 5% of subset weight\n",
    "    stable_pred1_sorted['cumulative_weight'] = stable_pred1_sorted['weight'].cumsum()\n",
    "    stable_pred1_final = stable_pred1_sorted[stable_pred1_sorted['cumulative_weight'] <= 0.05].copy()\n",
    "    \n",
    "    final_weight = stable_pred1_final['weight'] / stable_pred1_final['weight'].sum()\n",
    "    final_weighted_stability = final_weight * stable_pred1_final['stability_index']\n",
    "    \n",
    "    stable_pred1_final['final_weight'] = final_weight\n",
    "    stable_pred1_final['weighted_stability'].update(final_weighted_stability)\n",
    "    \n",
    "    print(stable_pred1_final.true_label.value_counts())\n",
    "    \n",
    "    return stable_pred1_final \n",
    "\n",
    "\n",
    "\n",
    "# Run this function\n",
    "# This function returns 1 user-level dataframe\n",
    "# It represents the final predicted subset (i.e. predicted subset AFTER filtering)\n",
    "def filter_predicted_subset(subset, subset_type):\n",
    "    \n",
    "    if subset_type == 'growth':\n",
    "        subset_final = filter_predicted_growth_subset(subset)\n",
    "        \n",
    "    else:\n",
    "        subset_final = filter_predicted_stable_subset(subset)\n",
    "    \n",
    "    return subset_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37e86d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    545\n",
      "1    157\n",
      "Name: true_label, dtype: int64\n",
      "0    251\n",
      "1     10\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "growth_pred_gbm_final = filter_predicted_subset(growth_pred_gbm, 'growth')\n",
    "stable_pred_gbm_final = filter_predicted_subset(stable_pred_gbm, 'stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af54ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    518\n",
      "1    159\n",
      "Name: true_label, dtype: int64\n",
      "0    302\n",
      "1     10\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "growth_pred_xgb_final = filter_predicted_subset(growth_pred_xgb, 'growth')\n",
    "stable_pred_xgb_final = filter_predicted_subset(stable_pred_xgb, 'stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6da9e862-eb8e-4b64-8c7a-61e4df539897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    894\n",
      "1    104\n",
      "Name: true_label, dtype: int64\n",
      "0    1384\n",
      "1      12\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# LogReg\n",
    "growth_pred_logreg_final = filter_predicted_subset(growth_pred_logreg, 'growth')\n",
    "stable_pred_logreg_final = filter_predicted_subset(stable_pred_logreg, 'stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "daf70bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    889\n",
      "1    135\n",
      "Name: true_label, dtype: int64\n",
      "0    391\n",
      "1      7\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "growth_pred_mlp_final = filter_predicted_subset(growth_pred_mlp, 'growth')\n",
    "stable_pred_mlp_final = filter_predicted_subset(stable_pred_mlp, 'stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0fe17207-fa58-4120-9d36-ed0add19ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1113\n",
      "1     456\n",
      "Name: true_label, dtype: int64\n",
      "0    216\n",
      "1      6\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Voting\n",
    "growth_pred_voting_final = filter_predicted_subset(growth_pred_voting, 'growth')\n",
    "stable_pred_voting_final = filter_predicted_subset(stable_pred_voting, 'stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d5c7b714-bb74-45af-b30d-d38b26826da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    826\n",
      "1    382\n",
      "Name: true_label, dtype: int64\n",
      "0    189\n",
      "1      9\n",
      "Name: true_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "growth_pred_stacking_final = filter_predicted_subset(growth_pred_stacking, 'growth')\n",
    "stable_pred_stacking_final = filter_predicted_subset(stable_pred_stacking, 'stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78c05a",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855349cb",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We would use the growth rate and stability index of the entire portfolio of the next 180 days actual data as the baseline benchmark for our evaluation. This is the bare minimum as our results should exceed this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the weighted stability index of subset\n",
    "def get_weighted_stability(n180d, df):\n",
    "\n",
    "    # Calculate stability_index with next 180 days data\n",
    "    result = n180d.groupby('user_id').agg(total_balance_std=('total_balance', 'std'),\n",
    "                                                  avg_balance=('total_balance', 'mean')).reset_index()\n",
    "\n",
    "    result['cv'] = result['total_balance_std'] / result['avg_balance']\n",
    "    result['cv_scaled'] = (result['cv'] - min(result['cv'])) / (max(result['cv']) - min(result['cv']))\n",
    "    result[\"stability_index\"] = 1 - result['cv_scaled']\n",
    "    \n",
    "    # sum of final_weight = 1\n",
    "    # use this weight to generate weighted stability\n",
    "    if 'final_weight' not in df.columns:\n",
    "        df['final_weight'] = df['weight'] / df['weight'].sum()\n",
    "    \n",
    "    result = df[['user_id', 'final_weight']].merge(result[['user_id', 'stability_index']], on='user_id', how='left')\n",
    "    result['weighted_stability'] = result['final_weight'] * result['stability_index']\n",
    "    \n",
    "    return result['weighted_stability'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f0ddd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (based on next 180 days actual data):\n",
      "Baseline growth rate:  -29.701 %\n",
      "Baseline stability index:  0.97596\n"
     ]
    }
   ],
   "source": [
    "# growth of entire portfolio over next 180 days actual data \n",
    "growth_baseline_subset = n180d.copy()\n",
    "growth_baseline_subset = growth_baseline_subset.groupby('pt_date')['total_balance'].sum().reset_index()\n",
    "baseline_growth = ((growth_baseline_subset.iloc[-1,1] - growth_baseline_subset.iloc[0,1]) / growth_baseline_subset.iloc[0,1])*100\n",
    "\n",
    "# stability index of entire portfolio over next 180 days actual data \n",
    "stable_baseline_subset = n180d.copy()\n",
    "stable_baseline_subset = stable_baseline_subset.groupby('user_id').agg(total_balance_std=('total_balance', 'std'),\n",
    "                                                  avg_balance=('total_balance', 'mean')).reset_index()\n",
    "stable_baseline_subset['weight'] = stable_baseline_subset['avg_balance'] / np.sum(stable_baseline_subset['avg_balance'])\n",
    "baseline_stability = get_weighted_stability(n180d, stable_baseline_subset)\n",
    "\n",
    "print('Baseline (based on next 180 days actual data):')\n",
    "print(f\"Baseline growth rate: \", round(baseline_growth, 3), \"%\")\n",
    "print(f\"Baseline stability index: \", round(baseline_stability, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb6141",
   "metadata": {},
   "source": [
    "## Average Benchmark\n",
    "\n",
    "This set of benchmark is based on a simple rule of ranking the users in descending order according to their growth coefficient and stability index from the last 90 days and filtering them by their cumulative weight till we hit a threshold of 5% weight of the entire last 90 days portfolio. Based on the respective set of users (growth and stable), we calculate their growth rate and stability index based on their next 180 days actual data and use these results as a benchmark for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69eb145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growth Subset (based on next 180 days actual data):\n",
      "Growth rate:  -26.12 %\n",
      "Stability index:  0.9782\n",
      "\n",
      "Stable Subset (based on next 180 days actual data):\n",
      "Growth rate:  -49.469 %\n",
      "Stability index:  0.98518\n"
     ]
    }
   ],
   "source": [
    "# subsetting for the stable and growth based on sorting their growth coefficient and stability index in ascending order\n",
    "growth_benchmark = growth_data.sort_values(by=['growth_coeff'], ascending=False)\n",
    "stable_benchmark = stable_data.sort_values(by=['stability_index'], ascending=False)\n",
    "\n",
    "# filter those who belongs to the 10k test set \n",
    "growth_benchmark = growth_benchmark[growth_benchmark['user_id'].isin(growth_test_users)]\n",
    "stable_benchmark = stable_benchmark[stable_benchmark['user_id'].isin(stable_test_users)]\n",
    "\n",
    "# filtering the users based on a threshold of 5% weight of entire portfolio\n",
    "growth_benchmark['weight'] = growth_benchmark['avg_balance'] / growth_benchmark['avg_balance'].sum()\n",
    "growth_benchmark['cumulative_weight'] = growth_benchmark['weight'].cumsum()\n",
    "growth_benchmark_subset = growth_benchmark[growth_benchmark['cumulative_weight'] <= 0.05]\n",
    "\n",
    "stable_benchmark['weight'] = stable_benchmark['avg_balance'] / stable_benchmark['avg_balance'].sum()\n",
    "stable_benchmark['cumulative_weight'] = stable_benchmark['weight'].cumsum()\n",
    "stable_benchmark_subset = stable_benchmark[stable_benchmark['cumulative_weight'] <= 0.05]\n",
    "\n",
    "# feature engineering on the 180 days data\n",
    "n180d_copy = n180d.copy()\n",
    "n180d_copy = n180d_copy.sort_values(by=['user_id', 'pt_date'])\n",
    "n180d_copy = n180d_copy.groupby('user_id').agg(total_balance_std=('total_balance', 'std'),\n",
    "                                                  avg_balance=('total_balance', 'mean'),\n",
    "                                              first_day_balance=('total_balance', 'first'),\n",
    "                                              last_day_balance=('total_balance', 'last')).reset_index()\n",
    "n180d_copy['weight'] = n180d_copy['avg_balance'] / n180d_copy['avg_balance'].sum()\n",
    "n180d_copy['cv'] = n180d_copy['total_balance_std'] / n180d_copy['avg_balance']\n",
    "n180d_copy['cv_scaled'] = (n180d_copy['cv'] - min(n180d_copy['cv'])) / (max(n180d_copy['cv']) - min(n180d_copy['cv']))\n",
    "n180d_copy[\"stability_index\"] = 1 - n180d_copy['cv_scaled']\n",
    "n180d_copy['weighted_stability'] = n180d_copy['weight'] * n180d_copy['stability_index']\n",
    "\n",
    "# growth and stable subset to have the 180 days for growth rate and stability calculation\n",
    "growth_benchmark_subset  = n180d_copy[n180d_copy['user_id'].isin(growth_benchmark_subset['user_id'])]\n",
    "stable_benchmark_subset = n180d_copy[n180d_copy['user_id'].isin(stable_benchmark_subset['user_id'])]\n",
    "\n",
    "# growth rate for benchmark growth subset \n",
    "first_day_balance = growth_benchmark_subset['first_day_balance'].sum()\n",
    "last_day_balance = growth_benchmark_subset['last_day_balance'].sum()\n",
    "growth_benchmark_growth = ((last_day_balance - first_day_balance) / first_day_balance)*100\n",
    "\n",
    "# growth rate for benchmark stable subset \n",
    "first_day_balance = stable_benchmark_subset['first_day_balance'].sum()\n",
    "last_day_balance = stable_benchmark_subset['last_day_balance'].sum()\n",
    "stable_benchmark_growth = ((last_day_balance - first_day_balance) / first_day_balance)*100\n",
    "\n",
    "print('Growth Subset (based on next 180 days actual data):')\n",
    "print(f\"Growth rate: \", round(growth_benchmark_growth, 3), \"%\")\n",
    "print(f\"Stability index: \", round(get_weighted_stability(n180d, growth_benchmark_subset), 5))\n",
    "\n",
    "print('\\nStable Subset (based on next 180 days actual data):')\n",
    "print(f\"Growth rate: \", round(stable_benchmark_growth, 3), \"%\")\n",
    "print(f\"Stability index: \", round(get_weighted_stability(n180d, stable_benchmark_subset), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea39835",
   "metadata": {},
   "source": [
    "## Ideal\n",
    "\n",
    "This ideal benchmark is the results we aim to achieve. This is based on the subsetting methodology that we use to label the users and train our classification models on (refer to the subsetting notebook file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83445b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growth Subset (based on next 180 days actual data):\n",
      "Growth rate:  40.068 %\n",
      "Stability index:  0.99172\n",
      "\n",
      "Stable Subset (based on next 180 days actual data):\n",
      "Growth rate:  1.184 %\n",
      "Stability index:  0.99977\n"
     ]
    }
   ],
   "source": [
    "# user_ids with label 1 from stable and growth data \n",
    "growth_ideal = growth_data[growth_data['label']==1]\n",
    "stable_ideal = stable_data[stable_data['label']==1]\n",
    "\n",
    "# feature engineering on the 180 days data\n",
    "n180d_copy = n180d.copy()\n",
    "n180d_copy = n180d_copy.sort_values(by=['user_id', 'pt_date'])\n",
    "n180d_copy = n180d_copy.groupby('user_id').agg(total_balance_std=('total_balance', 'std'),\n",
    "                                                  avg_balance=('total_balance', 'mean'),\n",
    "                                              first_day_balance=('total_balance', 'first'),\n",
    "                                              last_day_balance=('total_balance', 'last')).reset_index()\n",
    "n180d_copy['weight'] = n180d_copy['avg_balance'] / n180d_copy['avg_balance'].sum()\n",
    "n180d_copy['cv'] = n180d_copy['total_balance_std'] / n180d_copy['avg_balance']\n",
    "n180d_copy['cv_scaled'] = (n180d_copy['cv'] - min(n180d_copy['cv'])) / (max(n180d_copy['cv']) - min(n180d_copy['cv']))\n",
    "n180d_copy[\"stability_index\"] = 1 - n180d_copy['cv_scaled']\n",
    "n180d_copy['weighted_stability'] = n180d_copy['weight'] * n180d_copy['stability_index']\n",
    "\n",
    "# growth and stable subset to have the 180 days for growth rate and stability calculation\n",
    "growth_ideal_subset  = n180d_copy[n180d_copy['user_id'].isin(growth_ideal['user_id'])]\n",
    "stable_ideal_subset = n180d_copy[n180d_copy['user_id'].isin(stable_ideal['user_id'])]\n",
    "\n",
    "# growth rate for ideal growth subset \n",
    "first_day_balance = growth_ideal_subset['first_day_balance'].sum()\n",
    "last_day_balance = growth_ideal_subset['last_day_balance'].sum()\n",
    "growth_ideal_growth = ((last_day_balance - first_day_balance) / first_day_balance)*100\n",
    "\n",
    "# growth rate for ideal stable subset \n",
    "first_day_balance = stable_ideal_subset['first_day_balance'].sum()\n",
    "last_day_balance = stable_ideal_subset['last_day_balance'].sum()\n",
    "stable_ideal_growth = ((last_day_balance - first_day_balance) / first_day_balance)*100\n",
    "\n",
    "print('Growth Subset (based on next 180 days actual data):')\n",
    "print(f\"Growth rate: \", round(growth_ideal_growth, 3), \"%\")\n",
    "print(f\"Stability index: \", round(get_weighted_stability(n180d, growth_ideal_subset), 5))\n",
    "\n",
    "print('\\nStable Subset (based on next 180 days actual data):')\n",
    "print(f\"Growth rate: \", round(stable_ideal_growth, 3), \"%\")\n",
    "print(f\"Stability index: \", round(get_weighted_stability(n180d, stable_ideal_subset), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f87bb",
   "metadata": {},
   "source": [
    "# Evaluation and Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7401e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in 2 user-level dataframes of actual & predicted subset respectively\n",
    "# It maps and aggregates the total balance of all users in the subset for each day\n",
    "# Returns 2 dataframes in time series format (pt_date & total_balance), \n",
    "# one for actual subset and one for predicted subset\n",
    "def get_plotting_df(subset_pred, subset_benchmark):\n",
    "    \n",
    "    subset_pred_users = subset_pred.user_id.tolist()\n",
    "    subset_benchmark_users = subset_benchmark.user_id.tolist()\n",
    "\n",
    "    subset_pred_270 = df_270[df_270['user_id'].isin(subset_pred_users)]\\\n",
    "    .groupby('pt_date')['total_balance'].sum().reset_index()\n",
    "\n",
    "    subset_benchmark_270 = df_270[df_270['user_id'].isin(subset_benchmark_users)]\\\n",
    "    .groupby('pt_date')['total_balance'].sum().reset_index() \n",
    "\n",
    "    return subset_pred_270, subset_benchmark_270\n",
    "\n",
    "\n",
    "\n",
    "# This function plots the predicted subset against the actual subset\n",
    "# With the time series dataframes obtained from the above function\n",
    "def plot_pred_against_actual(subset_pred_270, subset_benchmark_270, \n",
    "                             subset_pred, subset_benchmark,\n",
    "                             subset_type):\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    # Plot the lines for each column\n",
    "    plt.plot(subset_pred_270['pt_date'], subset_pred_270['total_balance'], label=f\"Predicted {subset_type} subset\")\n",
    "    plt.plot(subset_benchmark_270['pt_date'], subset_benchmark_270['total_balance'], label=f\"Benchmark {subset_type} subset\")\n",
    "\n",
    "    # Set labels for the axes and the legend\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Balance')\n",
    "    plt.title(f'Predicted vs Benchmark {subset_type} Subset Balances Over Time')\n",
    "    plt.xticks(np.arange(0, len(subset_pred_270['pt_date']), 30), rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.axvline(x='2023-03-01', color='r', linestyle='--', label='End of last 90 days')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ###################\n",
    "    \n",
    "    num_users_pred = len(subset_pred.user_id.tolist())\n",
    "    num_users_baseline = len(n180d['user_id'].unique())\n",
    "    num_users_benchmark = len(subset_benchmark)\n",
    "    if subset_type == 'growth':\n",
    "        num_users_ideal = len(growth_data[growth_data['label']==1])\n",
    "    else:\n",
    "        num_users_ideal = len(stable_data[stable_data['label']==1])\n",
    "\n",
    "    weight_pred = subset_pred.weight.sum()\n",
    "    weight_baseline = 1\n",
    "    weight_benchmark = subset_benchmark.weight.sum()\n",
    "    if subset_type == 'growth':\n",
    "        weight_ideal = growth_ideal_subset['weight'].sum()\n",
    "    else:\n",
    "        weight_ideal = stable_ideal_subset['weight'].sum()\n",
    "\n",
    "\n",
    "    # Last 90 days metrics\n",
    "    weighted_stability_pred = subset_pred.weighted_stability.sum()\n",
    "    weighted_stability_true = subset_benchmark.weighted_stability.sum()\n",
    "    \n",
    "    starting_bal_pred = subset_pred.first_day_balance.sum()\n",
    "    ending_bal_pred = subset_pred.last_day_balance.sum()\n",
    "    growth_pred = (ending_bal_pred - starting_bal_pred) / starting_bal_pred\n",
    "    \n",
    "    \n",
    "    # Next 180 days metrics\n",
    "    growth_pred_n180 = (subset_pred_270.iloc[-1]['total_balance'] - subset_pred_270.iloc[90]['total_balance']) / subset_pred_270.iloc[90]['total_balance']\n",
    "    \n",
    "    weighted_stability_pred_n180 = get_weighted_stability(n180d, subset_pred)\n",
    "    \n",
    "    ####################\n",
    "    # Predicted subset #\n",
    "    ####################\n",
    "    \n",
    "    print(f\"Predicted {subset_type} subset (based on last 90 days data):\")\n",
    "    print(f\"Weight: \", round(weight_pred,5))\n",
    "    print(f\"No. of users: \", num_users_pred)\n",
    "    \n",
    "    print(f\"Growth rate: \", round(growth_pred_n180*100, 3), \"%\")\n",
    "    print(f\"Stability index: \", round(weighted_stability_pred_n180, 5))\n",
    "\n",
    "    \n",
    "    #################\n",
    "    #    Baseline   #\n",
    "    #################\n",
    "\n",
    "    print(f'\\nBaseline (based on next 180 days data):')\n",
    "    print(f\"Weight: \", round(weight_baseline,5))\n",
    "    print(f\"No. of users: \", num_users_baseline)\n",
    "    \n",
    "    print(f\"Growth rate: \", round(baseline_growth, 3), \"%\")\n",
    "    print(f\"Stability index: \", round(baseline_stability, 5))\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    #   Benchmark   #\n",
    "    #################\n",
    "    \n",
    "    print(f\"\\nBenchmark {subset_type} subset (based on next 180 days data):\")\n",
    "    print(f\"Weight: \", round(weight_benchmark,5))\n",
    "    print(f\"No. of users: \", num_users_benchmark)\n",
    "    \n",
    "    if subset_type =='growth':\n",
    "        print(f\"Growth rate: \", round(growth_benchmark_growth, 3), \"%\")\n",
    "        print(f\"Stability index: \", round(get_weighted_stability(n180d, growth_benchmark_subset), 5))\n",
    "    else:\n",
    "        print(f\"Growth rate: \", round(stable_benchmark_growth, 3), \"%\")\n",
    "        print(f\"Stability index: \", round(get_weighted_stability(n180d, stable_benchmark_subset), 5))\n",
    "    \n",
    "    #################\n",
    "    #    ideal     #\n",
    "    #################\n",
    "\n",
    "    print(f'\\nIdeal {subset_type} (based on next 180 days data):')\n",
    "    print(f\"Weight: \", round(weight_ideal,5))\n",
    "    print(f\"No. of users: \", num_users_ideal)\n",
    "\n",
    "    if subset_type =='growth':\n",
    "        print(f\"Growth rate: \", round(growth_ideal_growth, 3), \"%\")\n",
    "        print(f\"Stability index: \", round(get_weighted_stability(n180d, growth_ideal_subset), 5))\n",
    "    else:\n",
    "        print(f\"Growth rate: \", round(stable_ideal_growth, 3), \"%\")\n",
    "        print(f\"Stability index: \", round(get_weighted_stability(n180d, stable_ideal_subset), 5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7290a3",
   "metadata": {},
   "source": [
    "### Plot of actual vs predicted subsets BEFORE final filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27923cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can uncomment to see\n",
    "# LightGBM\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_gbm, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_gbm, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_gbm, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_gbm, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "\n",
    "# Add XGB code here\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_xgb, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_xgb, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_xgb, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_xgb, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# LogReg\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_logreg, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_logreg, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_logreg, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_logreg, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "\n",
    "# MLP\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_mlp, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_mlp, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_mlp, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_mlp, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# Voting\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_voting, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_voting, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_voting, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_voting, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# Stacking\n",
    "# growth_pred_270, growth_benchmark_270 = get_plotting_df(growth_pred_stacking, growth_benchmark_subset)\n",
    "# stable_pred_270, stable_benchmark_270 = get_plotting_df(stable_pred_stacking, stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_270, growth_benchmark_270, \n",
    "#                          growth_pred_stacking, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_270, stable_benchmark_270,  \n",
    "#                          stable_pred_stacking, stable_benchmark_subset, \n",
    "#                          'stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75bc4e",
   "metadata": {},
   "source": [
    "### Plot of actual vs predicted subsets AFTER final filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eb22e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# growth_pred_gbm_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_gbm_final, growth_benchmark_subset)\n",
    "# stable_pred_gbm_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_gbm_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_gbm_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_gbm_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_gbm_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_gbm_final, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# XGB\n",
    "# growth_pred_xgb_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_xgb_final, growth_benchmark_subset)\n",
    "# stable_pred_xgb_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_xgb_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_xgb_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_xgb_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_xgb_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_xgb_final, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "#LogReg\n",
    "# growth_pred_logreg_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_logreg_final, growth_benchmark_subset)\n",
    "# stable_pred_logreg_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_logreg_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_logreg_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_logreg_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_logreg_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_logreg_final, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# MLP\n",
    "# growth_pred_mlp_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_mlp_final, growth_benchmark_subset)\n",
    "# stable_pred_mlp_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_mlp_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_mlp_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_mlp_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_mlp_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_mlp_final, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# # Voting\n",
    "# growth_pred_voting_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_voting_final, growth_benchmark_subset)\n",
    "# stable_pred_voting_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_voting_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_voting_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_voting_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_voting_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_voting_final, stable_benchmark_subset, \n",
    "#                          'stable')\n",
    "\n",
    "# Stacking\n",
    "# growth_pred_stacking_270_final, growth_benchmark_270 = get_plotting_df(growth_pred_stacking_final, growth_benchmark_subset)\n",
    "# stable_pred_stacking_270_final, stable_benchmark_270 = get_plotting_df(stable_pred_stacking_final,  stable_benchmark_subset)\n",
    "\n",
    "# plot_pred_against_actual(growth_pred_stacking_270_final, growth_benchmark_270,\n",
    "#                          growth_pred_stacking_final, growth_benchmark_subset, \n",
    "#                          'growth')\n",
    "\n",
    "# plot_pred_against_actual(stable_pred_stacking_270_final, stable_benchmark_270,  \n",
    "#                          stable_pred_stacking_final, stable_benchmark_subset, \n",
    "#                          'stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export subset users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(best_model, subset_type):\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # lightgbm model\n",
    "    if best_model == 'lgbm' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_gbm_final['user_id']\n",
    "    elif best_model == 'lgbm' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_gbm_final['user_id']\n",
    "    # xgboost model\n",
    "    elif best_model == 'xgb' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_xgb_final['user_id']\n",
    "    elif best_model == 'xgb' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_xgb_final['user_id']\n",
    "    # logreg \n",
    "    elif best_model == 'logreg' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_logreg_final['user_id']\n",
    "    elif best_model == 'logreg' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_logreg_final['user_id']\n",
    "    # mlp \n",
    "    elif best_model == 'mlp' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_mlp_final['user_id']\n",
    "    elif best_model == 'mlp' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_mlp_final['user_id']\n",
    "    # voting\n",
    "    elif best_model == 'voting' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_voting_final['user_id']\n",
    "    elif best_model == 'voting' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_voting_final['user_id']\n",
    "    # stacking\n",
    "    elif best_model == 'stacking' and subset_type == 'growth':\n",
    "        result['user_id'] = growth_pred_stacking_final['user_id']\n",
    "    elif best_model == 'stacking' and subset_type == 'stable':\n",
    "        result['user_id'] = stable_pred_stacking_final['user_id']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid combination of best_model and subset_type\")\n",
    "\n",
    "    print('user ids exported to ' + best_model + '_' + subset_type + '_' + 'userid.csv')\n",
    "    # Export to CSV\n",
    "    result.to_csv(best_model + '_' + subset_type + '_' + 'userid.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user ids exported to logreg_growth_userid.csv\n"
     ]
    }
   ],
   "source": [
    "# user id of growth subset for stacking\n",
    "\n",
    "#inputs: best_model: [lgbm, xgb, logreg, mlp, voting, stacking]\n",
    "#        subset_type: [table, growth]\n",
    "\n",
    "export('logreg', 'growth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
